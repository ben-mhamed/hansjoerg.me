---
title: 'Contrasts in ANOVA and Regression'
author: 'Hansjörg Plieninger'
date: '2018-05-03'
slug: contrasts-in-anova-and-regression
categories:
  - R
draft: true
tags:
  - R
  - regression
  - ANOVA
  - SPSS
bibliography: ../../misc/lib.bib
csl: ../../misc/apa.csl
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
---


<div id="TOC">
<ul>
<li><a href="#example-data">Example data</a></li>
<li><a href="#anova-and-spss-perspective">ANOVA and SPSS Perspective</a></li>
<li><a href="#regression-perspective">Regression Perspective</a></li>
<li><a href="#how-to-combine-the-perspectives">How to Combine the Perspectives?</a></li>
<li><a href="#solution">Solution</a><ul>
<li><a href="#examples">Examples</a><ul>
<li><a href="#dummy-coding">Dummy Coding</a></li>
<li><a href="#planned-comparisonscontrast-coding">Planned Comparisons/Contrast Coding</a></li>
<li><a href="#helmert-coding">Helmert Coding</a></li>
</ul></li>
</ul></li>
<li><a href="#orthogonal-and-nonorthognoal-contrasts">Orthogonal and Nonorthognoal Contrasts</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>You can look at the problem of predicting a dependent variable using (categorical) independent variables from a regression perspective or from an ANOVA perspective, and I usually prefer the former. However, what I always liked about the ANOVA perspective was the focus on meaningful coding schemes for categorical predictors with more than two levels.</p>
<div id="example-data" class="section level2">
<h2>Example data</h2>
<p>Participants watched one of four different films and their positive affect (“PA2”) was assessed afterwards. To investigate the effect of films on affect, the categorical predictor has to be recoded into 3 coding variables.</p>
<pre class="r"><code>library(dplyr)
# library(fractional)
# library(codingMatrices)
# library(ggplot2)

data(affect, package = &quot;psych&quot;)
# help(affect, package = &quot;psych&quot;)

affect$Film &lt;- factor(affect$Film, labels = c(&quot;documentary&quot;, &quot;horror&quot;, &quot;nature&quot;, &quot;comedy&quot;))

# Sample subgroups of equal size (n=50)
set.seed(123)
dat &lt;- affect %&gt;% 
    group_by(Film) %&gt;% 
    sample_n(50)

table(dat$Film)
#&gt; 
#&gt; documentary      horror      nature      comedy 
#&gt;          50          50          50          50

(group_means &lt;- tapply(dat$PA2, dat$Film, mean))
#&gt; documentary      horror      nature      comedy 
#&gt;       7.612       7.460       7.960      12.090</code></pre>
</div>
<div id="anova-and-spss-perspective" class="section level2">
<h2>ANOVA and SPSS Perspective</h2>
<p>In the ANOVA world, I can express my hypotheses using three coding vectors as depicted in the Table <a href="#tab:contrasts1">1</a>. For example, I may investigate the difference between the last group “comedy” and the three other groups (H1). Furthermore, I may test the difference between “horror” vs. “documentary”+“nature” (H2). Lastly, I may test the difference between “documentary” and “nature”.</p>
<table>
<caption><span id="tab:contrasts1">Table 1: </span>ANOVA Contrasts</caption>
<thead>
<tr class="header">
<th align="left">Hypothesis</th>
<th align="right">c1</th>
<th align="right">c2</th>
<th align="right">c3</th>
<th align="right">c4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">H1</td>
<td align="right">-1</td>
<td align="right">-1</td>
<td align="right">-1</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="left">H2</td>
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">H3</td>
<td align="right">-1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>Furthermore, one of the very few things that I like about SPSS is the fact that I can relatively easily define such contrasts using <code>UNIANOVA</code>:</p>
<pre class="r"><code>### SPSS Syntax ###
UNIANOVA PA2 BY Film
  /CONTRAST(Film) = SPECIAL(-1 -1 -1  3
                             1 -2  1  0
                            -1  0  1  0).</code></pre>
</div>
<div id="regression-perspective" class="section level2">
<h2>Regression Perspective</h2>
<p>In regression, we often deal with categorical predictors as well, and often a first choice is dummy coding (the default in R for unordered factors). We all know how dummy coding works. Here, we choose “comedy” as a reference category, and all other categories are contrasted with that reference category using the scheme depicted in Table <a href="#tab:dummy">2</a>. The estimates give us the difference between “comedy” and each of the three other groups.</p>
<table>
<caption><span id="tab:dummy">Table 2: </span>Dummy Coding</caption>
<thead>
<tr class="header">
<th align="left">Group</th>
<th align="right">d1</th>
<th align="right">d2</th>
<th align="right">d3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">horror</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">nature</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">comedy</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">documentary</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</div>
<div id="how-to-combine-the-perspectives" class="section level2">
<h2>How to Combine the Perspectives?</h2>
<p>Even though I was comfortable using either approach, the thing that always bugged me was that I personally wasn’t able to fully bridge the two worlds. For example:</p>
<ol style="list-style-type: decimal">
<li>How can we use such schemes within the other framework? For example, plugging the dummy codes from Table <a href="#tab:dummy">2</a> into SPSS’s UNIANOVA Syntax won’t work.</li>
<li>How do we actually know the meaning of our estimates? For example, everybody knows that the estimate for <code>d1</code> above is the difference between “documentary” and “comedy”. But why is this the case?</li>
</ol>
</div>
<div id="solution" class="section level2">
<h2>Solution</h2>
<p>The solution is simple, but:</p>
<pre class="r"><code>fortunes::fortune(&quot;done it.&quot;)
#&gt; 
#&gt; It was simple, but you know, it&#39;s always simple when you&#39;ve done it.
#&gt;    -- Simone Gabbriellini (after solving a problem with a trick suggested
#&gt;       on the list)
#&gt;       R-help (August 2005)</code></pre>
<p>The solution is that the coding scheme used in SPSS above directly defines the <strong>contrast matrix</strong> <span class="math inline">\(\mathbf{C}\)</span> and this allows us to contrasts group means in a sensible way. On the other hand, the dummy-coding scheme used above was a <strong>coding matrix</strong> <span class="math inline">\(\mathbf{B}\)</span> <span class="citation">(Venables, 2018)</span>. These are two different things and they are related in the following way: <span class="math display">\[\beta = \mathbf{C} \mu = \mathbf{B}^{-1} \mu.\]</span> That is, the estimates <span class="math inline">\(\beta\)</span> are the reparameterized group means <span class="math inline">\(\mu\)</span>. The contrasts in <span class="math inline">\(\mathbf{C}\)</span> directly specify the weights of the groups and are easily interpretable. However, the interpretation of the codes in <span class="math inline">\(\mathbf{B}\)</span>, for example, of the dummy codes used above, is not directly given, but only through the inverse of <span class="math inline">\(\mathbf{B}\)</span>.</p>
<div id="examples" class="section level3">
<h3>Examples</h3>
<div id="dummy-coding" class="section level4">
<h4>Dummy Coding</h4>
<p>Here, we use the default in R, namely, dummy coding, for the affect data. Remember that we want to investigate whether positive affect (“PA2”) differs between groups that watched one of four movies. The interpretation of the results is straightforward: The intercept is equal to the first group mean, and here it is significantly different from zero. Furthermore, we observe that “comedy” in comparison to the reference category “documentary” leads to higher levels of positive effect (the difference is 4.478, p = .0001).</p>
<pre class="r"><code>contr.treatment(levels(dat$Film))
#&gt;             horror nature comedy
#&gt; documentary      0      0      0
#&gt; horror           1      0      0
#&gt; nature           0      1      0
#&gt; comedy           0      0      1

summary(lm(PA2 ~ Film, data = dat))
#&gt; 
#&gt; Call:
#&gt; lm(formula = PA2 ~ Film, data = dat)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -12.090  -4.612  -0.536   3.942  18.040 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)   7.6120     0.8058   9.446  &lt; 2e-16 ***
#&gt; Filmhorror   -0.1520     1.1396  -0.133 0.894029    
#&gt; Filmnature    0.3480     1.1396   0.305 0.760407    
#&gt; Filmcomedy    4.4780     1.1396   3.929 0.000118 ***
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 5.698 on 196 degrees of freedom
#&gt; Multiple R-squared:  0.1038, Adjusted R-squared:  0.09005 
#&gt; F-statistic: 7.564 on 3 and 196 DF,  p-value: 8.184e-05

group_means
#&gt; documentary      horror      nature      comedy 
#&gt;       7.612       7.460       7.960      12.090

12.090 - 7.612       # == &#39;Filmhorror&#39;
#&gt; [1] 4.478</code></pre>
<p>But why exactly are dummy codes interpreted in this way? We learned in stats 101 how to interpret the <span class="math inline">\(\mathbf{B}\)</span> matrix depicted in Table <a href="#tab:dummy">2</a> (i.e., <code>contr.treatment()</code>). But to really deduce the meaning, you have to take the inverse of the coding matrix <span class="math inline">\(\mathbf{B}\)</span>. (The package <strong>codingMatrices</strong> has a wrapper around <code>solve()</code> with nice output, but I will use <code>solve()</code> here for transparency.)</p>
<pre class="r"><code># we need to add a vector of 1&#39;s for the intercept
solve(cbind(Ave = 1, contr.treatment(4)))
#&gt;      1 2 3 4
#&gt; Ave  1 0 0 0
#&gt; 2   -1 1 0 0
#&gt; 3   -1 0 1 0
#&gt; 4   -1 0 0 1

# identical result but nicer ouput:
# codingMatrices::mean_contrasts(contr.treatment(4))</code></pre>
<p>This returns the contrast matrix <span class="math inline">\(\mathbf{C}\)</span>. The first row gives you the interpretation of the intercept, and we see that it is <span class="math inline">\(\beta_0 = 1\mu_1 + 0\mu_2 + 0\mu_3 + 0\mu_4\)</span>. Likewise, from the second row, <span class="math inline">\(\beta_1 = -1\mu_1 + 1\mu_2\)</span>; and so on. The interpretation of this <span class="math inline">\(\mathbf{C}\)</span> matrix is much easier than that of the <span class="math inline">\(\mathbf{B}\)</span> matrix, which was only easy because we learned it by hard.</p>
<p>Lastly, this enables us to use dummy coding (or any other coding scheme that you learned by hard) in SPSS’s <code>UNIANOVA</code>:</p>
<pre class="r"><code>### SPSS Syntax ###
UNIANOVA PA2 BY Film
  /CONTRAST(Film) = SPECIAL(-1  1  0  0
                            -1  0  1  0
                            -1  0  0  1).</code></pre>
</div>
<div id="planned-comparisonscontrast-coding" class="section level4">
<h4>Planned Comparisons/Contrast Coding</h4>
<p>The whole story gets more interesting when trying to implement non-standard coding schemes within the regression framework. Remember the hypotheses described in Table <a href="#tab:contrasts1">1</a> above; those are easily translated into a contrast matrix <span class="math inline">\(\mathbf{C}\)</span> called <code>C1</code> below. In order to test the hypotheses within a regression framework, we have to calculate the <span class="math inline">\(\mathbf{B}\)</span> matrix (<code>B1</code>). Furthermore, instead of using integral weights, I divide by the number of non-zero weights for easier interpretation of the estimates (but p-values are the same).</p>
<pre class="r"><code>tmp1 &lt;- matrix(c( 1,  1,  1, 1,
                 -1, -1, -1, 3,
                 -1,  2, -1, 0,
                 -1,  0,  1, 0), 4, 4, byrow = TRUE)
C1 &lt;- tmp1 / (4:1)
tmp1
#&gt;      [,1] [,2] [,3] [,4]
#&gt; [1,]    1    1    1    1
#&gt; [2,]   -1   -1   -1    3
#&gt; [3,]   -1    2   -1    0
#&gt; [4,]   -1    0    1    0
round(C1, 2)
#&gt;       [,1]  [,2]  [,3] [,4]
#&gt; [1,]  0.25  0.25  0.25 0.25
#&gt; [2,] -0.33 -0.33 -0.33 1.00
#&gt; [3,] -0.50  1.00 -0.50 0.00
#&gt; [4,] -1.00  0.00  1.00 0.00

B1 &lt;- solve(C1)
round(B1, 2)
#&gt;      [,1]  [,2]  [,3] [,4]
#&gt; [1,]    1 -0.25 -0.33 -0.5
#&gt; [2,]    1 -0.25  0.67  0.0
#&gt; [3,]    1 -0.25 -0.33  0.5
#&gt; [4,]    1  0.75  0.00  0.0

colnames(B1) &lt;- paste0(&quot;_Hyp&quot;, 0:3)</code></pre>
<p>Finally, we can use the <span class="math inline">\(\mathbf{B}\)</span> matrix to specify the desired contrasts in a linear regression:</p>
<pre class="r"><code>summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = B1[, -1])))
#&gt; 
#&gt; Call:
#&gt; lm(formula = PA2 ~ Film, data = dat, contrasts = list(Film = B1[, 
#&gt;     -1]))
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -12.090  -4.612  -0.536   3.942  18.040 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)   8.7805     0.4029  21.793  &lt; 2e-16 ***
#&gt; Film_Hyp1     4.4127     0.9305   4.742 4.06e-06 ***
#&gt; Film_Hyp2    -0.3260     0.9869  -0.330    0.742    
#&gt; Film_Hyp3     0.3480     1.1396   0.305    0.760    
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 5.698 on 196 degrees of freedom
#&gt; Multiple R-squared:  0.1038, Adjusted R-squared:  0.09005 
#&gt; F-statistic: 7.564 on 3 and 196 DF,  p-value: 8.184e-05

mean(group_means)
#&gt; [1] 8.7805
group_means[[4]] - mean(group_means[1:3])
#&gt; [1] 4.412667
group_means[[2]] - mean(group_means[c(1, 3)])
#&gt; [1] -0.326
group_means[[3]] - group_means[[1]]
#&gt; [1] 0.348</code></pre>
<p>As you can see, the following holds:</p>
<ul>
<li><span class="math inline">\(\beta_0 = .25\mu_1 +.25\mu_2 +.25\mu_3 + .25\mu_4\)</span>,</li>
<li><span class="math inline">\(\beta_1 = -.33\mu_1 -.33\mu_2 -.33\mu_3 + \mu_4\)</span>,</li>
<li><span class="math inline">\(\beta_2 = -.5 \mu_1 + \mu_2 -.5\mu_3\)</span>,</li>
<li><span class="math inline">\(\beta_3 = - \mu_1 + \mu_3\)</span>.</li>
</ul>
<p>Thus, positive affect is 4.41 points higher after watching “comedy” compared to the three other films (p &lt; .001). Moreover, positive affect is 0.32 points lower after “horror” compared to groups 1 and 3, but this is not significant (p = .742). And the difference between “documentary”&quot; and “nature” is also non-significant (p = .760).</p>
<p>This gives us huge power. We can now test more specific hypotheses in a regression framework compared to what is possible using the software’s defaults; for example, the hypotheses depicted in Table <a href="#tab:contrasts1">1</a> were easy to test using our custom matrices <code>B1</code> and <code>C1</code> but would be hard to test with built-in functions. Furthermore, we know now how to deduce the meaning of the estimates instead of relying on textbook knowledge.</p>
</div>
<div id="helmert-coding" class="section level4">
<h4>Helmert Coding</h4>
<p>For further illustration, we will have a look at Helmert coding (<code>contr.helmert()</code>), which can be used to compare each level with the mean of the previous levels. The <span class="math inline">\(\mathbf{C}\)</span> matrix <code>C2</code> already illustrates that, but it does not give an interpretation for <span class="math inline">\(\beta_0\)</span> and it does not allow to interpret the exact value of the other estimates. This is given by its inverse <code>B2</code>, which shows that <span class="math inline">\(\beta_0\)</span> is again the mean of the group means (first row of <code>B2</code>). Furthermore, <span class="math inline">\(\beta_3\)</span> compares “comedy” to the three other groups (H1 in Table <a href="#tab:contrasts1">1</a>), and the p- and t-value (4.742) are the same as above. However, the estimate of 1.10 has no easy interpretation, because it is <span class="math inline">\(\beta_3 = -\frac{1}{12}\mu_1 - \frac{1}{12}\mu_2 - \frac{1}{12}\mu_3 + \frac{1}{4}\mu_4\)</span> (with <span class="math inline">\(\frac{1}{12} \approx 0.08\)</span>). This was much easier to interpret above, where the estimate was 4.41, which was just the difference between “comedy” and the mean of the other three groups.</p>
<pre class="r"><code>(C2 &lt;- contr.helmert(levels(dat$Film)))
#&gt;             [,1] [,2] [,3]
#&gt; documentary   -1   -1   -1
#&gt; horror         1   -1   -1
#&gt; nature         0    2   -1
#&gt; comedy         0    0    3

B2 &lt;- solve(cbind(1, C2))
round(B2, 2)
#&gt;      documentary horror nature comedy
#&gt; [1,]        0.25   0.25   0.25   0.25
#&gt; [2,]       -0.50   0.50   0.00   0.00
#&gt; [3,]       -0.17  -0.17   0.33   0.00
#&gt; [4,]       -0.08  -0.08  -0.08   0.25

summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = contr.helmert)))
#&gt; 
#&gt; Call:
#&gt; lm(formula = PA2 ~ Film, data = dat, contrasts = list(Film = contr.helmert))
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -12.090  -4.612  -0.536   3.942  18.040 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)   8.7805     0.4029  21.793  &lt; 2e-16 ***
#&gt; Film1        -0.0760     0.5698  -0.133    0.894    
#&gt; Film2         0.1413     0.3290   0.430    0.668    
#&gt; Film3         1.1032     0.2326   4.742 4.06e-06 ***
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 5.698 on 196 degrees of freedom
#&gt; Multiple R-squared:  0.1038, Adjusted R-squared:  0.09005 
#&gt; F-statistic: 7.564 on 3 and 196 DF,  p-value: 8.184e-05

sum(B2[4, ] * group_means)
#&gt; [1] 1.103167</code></pre>
</div>
</div>
</div>
<div id="orthogonal-and-nonorthognoal-contrasts" class="section level2">
<h2>Orthogonal and Nonorthognoal Contrasts</h2>
<p>I did not find much literature that deduces the meaning of the <span class="math inline">\(\textbf{B}\)</span> matrix and explains the relationship between <span class="math inline">\(\textbf{B}\)</span> and <span class="math inline">\(\textbf{C}\)</span> except <span class="citation">Venables (2018)</span>. Maybe because it is obvious for everyone except for me. But maybe because the interpretation of orthogonal contrasts is relatively easy, because then <span class="math inline">\(\textbf{B}\)</span> and <span class="math inline">\(\textbf{C}\)</span> are structurally quite similar:</p>
<pre class="r"><code># B matrix for Helmert coding
contr.helmert(5)
#&gt;   [,1] [,2] [,3] [,4]
#&gt; 1   -1   -1   -1   -1
#&gt; 2    1   -1   -1   -1
#&gt; 3    0    2   -1   -1
#&gt; 4    0    0    3   -1
#&gt; 5    0    0    0    4
tmp1 &lt;- solve(cbind(1, contr.helmert(5)))[-1, ]
# C matrix
round(t(tmp1), 2)
#&gt;   [,1]  [,2]  [,3]  [,4]
#&gt; 1 -0.5 -0.17 -0.08 -0.05
#&gt; 2  0.5 -0.17 -0.08 -0.05
#&gt; 3  0.0  0.33 -0.08 -0.05
#&gt; 4  0.0  0.00  0.25 -0.05
#&gt; 5  0.0  0.00  0.00  0.20</code></pre>
<p>However, this is not the case for nonorthogonal contrasts like dummy- or effect-coding or other custom, nonorthogonal contrasts. Apart from that, I can’t see any clear advantage of orthogonal over nonorthogonal contrasts. If you know better, I would be very happy if you could let me know.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-venables_coding_2018">
<p>Venables, B. (2018). <em>Coding matrices, contrast matrices and linear models</em>. Retrieved from <a href="https://cran.r-project.org/web/packages/codingMatrices/vignettes/" class="uri">https://cran.r-project.org/web/packages/codingMatrices/vignettes/</a></p>
</div>
</div>
</div>
