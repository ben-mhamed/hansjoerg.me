---
title: 'Tutorial: Rasch and 2PL Model in R'
author: 'HansjÃ¶rg Plieninger'
date: '2018-04-23'
slug: rasch-in-r-tutorial
summary: 'Recently, I wrote a summary of some illustrative IRT analyses for my students. Quickly, I realized that this might be of interest to others as well, and I am posting here a tutorial for the Rasch model and the 2PL model in R. It is meant for people with a basic understanding of these models who have heard terms like ICC or item difficulty before and who would like to see a practical, worked example. Possibly, the code may be copied and applied to your own data.'
categories:
  - R
tags:
  - IRT
  - psychometrics
  - R
  - tutorial
bibliography: ../../misc/lib.bib
csl: ../../misc/apa.csl
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", out.width = "80%")
```

Recently, I wrote a summary of some illustrative IRT analyses for my students. Quickly, I realized that this might be of interest to others as well, and I am posting here a tutorial for the Rasch model and the 2PL model in R. It is meant for people with a basic understanding of these models who have heard terms like *ICC* or *item difficulty* before and who would like to see a practical, worked example. Possibly, the code may be copied and applied to your own data. Let me know if there is anything that needs further clarification.

## Setup

Herein, we will use the following three R packages: **eRm** [@mair_extended_2007], **ltm** [@rizopoulos_ltm_2006], and **difR** [@magis_general_2010]. Those need to be loaded via `library()` and installed beforehand if necessary.

```{r library, warning = FALSE, message = FALSE}
# install.packages("eRm")
# install.packages("ltm")
# install.packages("difR")
library("eRm")
library("ltm")
library("difR")
```

## Data

An illustrative data set on *verbal aggression* will be used, which is comprised of 24 dichotomized items and 316 persons. The data set called verbal is included in the **difR** package and can be loaded using the function `data()`. Documentation for the data is available through `?verbal`.

```{r data}
data(verbal, package = "difR")
# ?verbal
```


```{r DT, echo = FALSE, message = FALSE, warning = FALSE}
# library(DT)
# library(widgetframe)
# dt <-  DT::datatable(verbal,
#                      extensions = "Scroller", 
#                      options = list(
#                          # pageLength = 17, autoWidth = F,
#                          # lengthMenu = c(1:9, 17),
#                          # dom = "ltpr",
#                          scrollX = TRUE,
#                          scrollY = 200,
#                          deferRender = TRUE,
#                          scroller = TRUE
#                          # colReorder = TRUE,
#                          # fixedColumns = list(leftColumns = 3),
#                          # buttons = I("colvis"),
#                          # buttons = list(list(extend = "colvis",
#                          #                     columns = 3:(1 + ncol(dat_3))))),
#   # head(iris, 20), 
#   # options = list(
#   #    columnDefs = list(list(className = 'dt-center', targets = 5)),
#   #    pageLength = 5, lengthMenu = c(5, 10, 15, 20)),
#   # fillContainer = T
#   ))
# 
# widgetframe::frameWidget(dt
#                          # , height = 350, width = '95%'
#                          )
```

## Rasch Model

A Rasch model is fit to the data using conditional maximum likelihood (CML) estimation of the item parameters as provided in the function `RM()` of the **eRm** package.

The item difficulty parameters are returned and the output shows that S2WantCurse is the easiest item and S3DoShout is the most difficult item, for which only people with high levels of verbal aggression are expected to agree with.

```{r, cache = FALSE}
dat_1 <- verbal[, 1:24]

res_rm_1 <- RM(dat_1)

# res_rm_1                   # Short summary of item parameters
# summary(res_rm_1)          # Longer summary of item parameters
betas <- -coef(res_rm_1)     # Item difficulty parameters
round(sort(betas), 2)
```

### Plots

We may plot the ICC of a single or all items, which is just a graphical illustration of the estimated item parameters.


```{r rasch-plots, cache = FALSE}
plotICC(res_rm_1, item.subset = "S2WantShout")
abline(v = -0.18, col = "grey")
abline(h = .5, col = "grey")
plotjointICC(res_rm_1, item.subset =  1:12, cex = .6)
plotjointICC(res_rm_1, item.subset = 13:24, cex = .6)
```

From the plots of the ICCs, it seems like the "want"-items (items 1:12) are easier than the "do"-items (items 13:24). We may further investigate this by comparing the means of the item difficulties. This shows that this is indeed the case. (The two means sum to 0, because of the identification constraint, see below.)

```{r}
mean(betas[ 1:12])           # Want-items
mean(betas[13:24])           # Do-items
```

Furthermore, a very informative plot called a person-item map is available in **eRm**:

> A person-item map displays the location of item (and threshold) parameters as well as the distribution of person parameters.along the latent dimension. Person-item maps are useful to compare the range and position of the item measure distribution (lower panel) to the range and position of the person measure distribution (upper panel). Items should ideally be located along the whole scale to meaningfully measure the 'ability' of all persons. (From `?plotPImap`)

```{r, cache = FALSE}
plotPImap(res_rm_1, cex.gen = .55)
plotPImap(res_rm_1, cex.gen = .55, sorted = TRUE)
```

### Model Identification

The model needs an identification constraint, because otherwise the location of the latent scale is undetermined. When looking at the help (`?RM`), we can see that the identification constraint in `RM()` is a sum-to-zero constraint by default (`sum0 = TRUE`). We can verify that by calculating the sum of the item parameters.

```{r, cache = FALSE}
round(sum(betas), 10)
```

When setting `sum0 = FALSE`, the first item parameter is set to zero. These are the only two options implemented in `RM()`.

```{r, cache = FALSE}
tmp1 <- RM(dat_1, sum0 = FALSE)
round(coef(tmp1), 2)
```

### Note on Item Parameters in eRm Package

Note that the **eRm** package works with easiness parameters, which are called beta, and those can be transformed into difficulty parameters, which are called eta, and $-\eta = \beta$.

Note further that often only $I-1$ item parameters are returned (here 23), which illustrates the fact that only 23 item parameters can be estimated and that one has to be fixed either as $\beta_1=\sum_2^I \beta_i$ (if `sum0 = TRUE`) or $\beta_1 = 0$ (if `sum0 = FALSE`). The first item parameter can then be calculated (not estimated) accordingly.

```{r}
# No difficulty (eta) parameter is returned for the first item S1wantCurse
summary(res_rm_1)

# Calculate eta for item S1wantCurse
-sum(res_rm_1$etapar)
```

### MML Estimation

In contrast to the CML estimation above, the model is now fit using marginal maximum likelihood (MML) estimation using the function `rasch()` from the **ltm** package. Therein, the model is identified by assuming a standard normal distribution of the person parameters. Thus, all item parameters can be freely estimated because the mean of the person parameters is fixed to 0 (which was not the case with CML).

```{r}
res_rm_2 <- rasch(dat_1)
res_rm_2
```

Note further that `rasch()` did not only fix the mean of the person-parameter distribution, but also the variance. Thus, the variability or units of the latent scale are determined by this. With CML estimation, this was fixed by assuming discrimination parameters equal to 1. However, two constraints (variance of person-parameter distribution as well as discrimination parameter) would be overly restrictive, and thus a common discrimination parameter is freely estimated by `rasch()`, which equals 1.37. Don't get distracted by these technical issues, the software takes care of it.

The difficulty parameters estimated via MML or CML are nearly identical here highlighting the fact that the normal-distribution assumption in MML did not do any harm.

```{r}
cor(coef(res_rm_2)[, 1], betas)
```

## 2PL Model

A 2PL model cannot be estimated with **eRm**, but with the package **ltm**, for example.

The estimated difficulty parameters (Dffclt) seem roughly comparable to those from the Rasch model. The discrimination parameters (Dscrmn), which were all equal in the Rasch model, show some but not massive heterogeneity. This is also evident from the plot of the ICCs.

Note that `res_2pl_1` and `res_rm_1` were estimated using two different packages. Thus, the structure of these objects is completely different and also the output if they are printed or the functions you may use with them (e.g., `plot()` vs. `plotICC()`).

```{r 2pl, cache = F}
# ltm() takes a formula with a tilde (~) as its first argument. The left-hand side
# must be the data and the right-hand side are the latent variables (only one
# here).
res_2pl_1 <- ltm(dat_1 ~ z1)
res_2pl_1

# ?plot.ltm
plot(res_2pl_1, items =  1:12)
plot(res_2pl_1, items = 13:24)
```

## Model Fit

### Relative Fit of Rasch and 2PL Model

To compare the Rasch and the 2PL model, both need to be fit using the same assumptions and the same algorithm. Therefore, the comparison is made here based on the models estimated via MML.

#### Likelihood-Ratio Test

Since the two models are nested, they can be compared using a Likelihood-Ratio (LR) test. Here, the test is significant (p = .015) indicating that both models do **not** fit equally well and that the more complex 2PL model is preferred.

Note that a model with more parameters always leads to a higher likelihood, which is also the case here. Note further, that the degrees of freedom (df) for the LR test equal the number of constrained parameters. In the 2PL model, 24 discrimination parameters were estimated, and in the Rasch model only 1, which gives df = 23.

```{r}
anova(res_rm_2, res_2pl_1)
```

#### AIC and BIC

To compare two models descriptively, they need not necessarily be nested. With respect to AIC and BIC, lower values indicate better fit. Herein, the Rasch model is preferred by both criteria.

#### Comparison of Item Parameters

To further evaluate differences between the two models, the difficulty parameters can be correlated across models. Again, the extremely high correlation indicates that the differences between models is small.

```{r}
cor(coef(res_rm_2)[, 1], coef(res_2pl_1)[, 1])
```

### Absolute Fit of the Rasch Model

In the remainder of this subsection, a few procedures for assessing the fit of the Rasch model will be illustrated.

Several of them are based on the idea that the item parameters---estimated separately in two groups---should be equal if the model holds. For illustrative purposes, the variable sex is used here but other/additional variables can of course be used.

#### Andersen's Likelihood-Ratio Test and Graphical Model Check

The item parameters should be equal if the model holds and thus be close to the line in the following plot. The ellipses indicate the confidence intervals, and many of them overlap the line indicating good fit. A few items stand out, for example item 6 (S2WantShout), which is easier for women than for men.

The significance test that formalizes this idea is called Andersen's Likelihood Ratio test [@andersen_goodness_1973]. Herein, it is significant indicating bad fit due to too many items with diverging item difficulties.

```{r, cache = FALSE}
lrt_1 <- LRtest(res_rm_1, splitcr = verbal$Gender)
plotGOF(lrt_1, conf = list(), tlab = "number",
        xlab = "Women", ylab = "Men")
lrt_1
```

Removing bad fitting items should improve fit, and this is indeed the case here if item 6 is removed: The test statistic is smaller now even though still significant.

```{r, cache = FALSE}
tmp1 <- RM(dat_1[, -6])
LRtest(tmp1, splitcr = verbal$Gender)
```

#### Wald Test

The Wald test is also based on the comparison of two groups; however it is used for individual items. Again, misfit is indicated for item 6 among others. It is interesting to note that the z-statistic is mostly negative for do-items and positive for want-items. Negative values indicate that the item is easier for the second group, namely, men. Thus, it seems easier for men compared to women to "do" or show verbal aggression.

```{r, cache = FALSE}
Waldtest(res_rm_1, splitcr = verbal$Gender)
```

#### Infit and Outfit

Infit and outfit statistics are not based on the comparison of groups, but on residuals. Descriptively, the MSQ values should be, for instance, within the range from 0.7 to 1.3  [@wright_reasonable_1994]. A t-test is also reported for infit and outfit, and the t-value should conventionally be within the range from -1.96 to +1.96. Here, the previously flagged item 6 performs quite good, but other items do not, for example item 14 (S1DoScold). The **eRm** package also provides a plot of the infit t-values, called a Pathway Map, which is also available for persons instead of items (i.e., `plotPWmap(res_rm_1, pmap = TRUE, imap = FALSE)`).

```{r, cache = FALSE}
# To calculate infit/outfit via itemfit(), the person parameters have to be
# estimated first
pp_ml_1 <- person.parameter(res_rm_1)
itemfit(pp_ml_1)

plotPWmap(res_rm_1)
```

### DIF

Herein, differential item functioning (DIF) or item bias or measurement invariance is treated as a topic within Model Fit, but it could also be treated as a topic of its own given the high relevance in the IRT literature.

Above, we already compared item parameters across groups from a fit perspective. However, if the groups are the focal and the reference group, these procedures can also be interpreted from a DIF perspective. Thus, the Wald test indicated that item 6 showed DIF, namely, uniform DIF.

#### Mantel-Haenszel

The Mantel-Haenszel method is a non-parametric approach to DIF. Herein, again item 6 is flagged having significant DIF (p = .002) with a large effect size of $\Delta_{MH}$=-2.49. The negative effect size indicates that the item is harder for the focal group, namely, men, mirroring the results from above. The plot gives a compact summary across all items.

```{r Mantel-Haenszel, cache = FALSE}
tmp1 <- difMH(dat_1, group = verbal$Gender, focal.name = 1)
tmp1

plot(tmp1)
```

#### Lord's Chi-Square-Test

Lord's $\chi^2$-Test allows to test for non-uniform (crossing) DIF, because a 2PL (or even 3PL) model is estimated in each group rather than a Rasch model, which was used for the Wald test.

Here, item 6 has still a relatively high $\chi^2$-value but it is non-significant (p = .075). The ICCs indicate that item 6 is a little bit harder for men and also a little bit less discriminating (uniform and non-uniform DIF)---but non-significant.

```{r Lord, cache = FALSE}
tmp1 <- difLord(dat_1, group = verbal$Gender, focal.name = 1, model = "2PL")
# tmp1 <- difLord(dat_1, group = verbal$Gender, focal.name = 1, model = "1PL", discr = NULL)
tmp1

plot(tmp1)
plot(tmp1, plot = "itemCurve", item = 6)
```

## Person Parameters

An ability parameter for every person can be estimated using different methods, and those will be illustrated and compared in the following.

### ML

Maximum likelihood (ML) estimation without further assumptions/information is available in the **eRm** package. ML estimation does not allow an estimate for persons who solved none or all items. Estimates are calculated via extrapolation for those persons per default (i.e., `coef(extrapolated = TRUE)`), but this can be disabled.

```{r, cache = FALSE}
tmp1 <- person.parameter(res_rm_1)
pp_ml <- coef(tmp1)
```

### MAP and EAP

Maximum a Posteriori (MAP) also known as Empirical Bayes (EB) estimates are available in the **ltm** package, as well as the closely related Expected a Posteriori (EAP) estimates. Both assume a normal distribution prior for the person parameters.

The extremely high correlation among the estimates indicates that the methods lead to the same conclusions for the present data set, and this will often be the case.

```{r, cache = FALSE}
pp_map <- factor.scores(res_rm_2, method = "EB", resp.patterns = dat_1)
pp_eap <- factor.scores(res_rm_2, method = "EAP", resp.patterns = dat_1)
tmp1 <- data.frame(ML = pp_ml, MAP = pp_map$score.dat$z1, EAP = pp_eap$score.dat$z1)
round(cor(tmp1), 4)
```

Furthermore, the scatter plot of the ML and the MAP estimates illustrates the effect of the prior. The relationship is linear for intermediate person parameters. However, shrinkage towards 0 is observed for extreme person parameters such that the MAP estimates are less extreme than the ML estimates.

```{r, cache = FALSE}
plot(tmp1[, 1:2])
```

Furthermore, we may compare the estimates for the Rasch and the 2PL model as a supplement to the model comparison from above. Again, the person parameters are almost identical highlighting again that the difference between the two models is not large for the present data set.

```{r, cache = FALSE}
pp_2pl <- factor.scores(res_2pl_1, method = "EB", resp.patterns = dat_1)
cor(pp_map$score.dat$z1, pp_2pl$score.dat$z1)
```

## Item and Test Information

Item information curves (IICs) are available in both the **eRm** (`plotINFO()`) and **ltm** (`plot(type = "IIC")`) package. Those curves are an illustration of the difficulty and discrimination parameters. Therefore, it is expected that maximum information is highest for the items with the highest discrimination parameter (e.g., item 14 S1DoScold). It is interesting to see that the do-items show generally larger discrimination parameters, which is often interpreted as being a closer or better measure of the latent variable.

```{r IIC, cache = FALSE}
res_2pl_1

plot(res_2pl_1, items =  1:12, type = "IIC", ylim = c(0, 1.3))
plot(res_2pl_1, items = 13:24, type = "IIC", ylim = c(0, 1.3))
plot(res_2pl_1, items = 0, type = "IIC")
```

## References
