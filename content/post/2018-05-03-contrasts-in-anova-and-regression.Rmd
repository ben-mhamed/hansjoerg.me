---
title: 'Contrasts in ANOVA and Regression'
author: 'HansjÃ¶rg Plieninger'
date: '2018-05-03'
slug: contrasts-in-anova-and-regression
categories:
  - R
draft: true
tags:
  - R
  - regression
  - ANOVA
  - SPSS
bibliography: ../../misc/lib.bib
csl: ../../misc/apa.csl
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
```

You can look at the problem of predicting a dependent variable using (categorical) independent variables from a regression perspective or from an ANOVA perspective, and I usually prefer the former. However, what I always liked about the ANOVA perspective was the focus on meaningful coding schemes for categorical predictors with more than two levels.

## Example data

Participants watched one of four different films and their positive affect ("PA2") was assessed afterwards. To investigate the effect of films on affect, the categorical predictor has to be recoded into 3 coding variables.

```{r, message = F}
library(dplyr)
# library(fractional)
# library(codingMatrices)
# library(ggplot2)

data(affect, package = "psych")
# help(affect, package = "psych")

affect$Film <- factor(affect$Film, labels = c("documentary", "horror", "nature", "comedy"))

# Sample subgroups of equal size (n=50)
set.seed(123)
dat <- affect %>% 
    group_by(Film) %>% 
    sample_n(50)

table(dat$Film)

(group_means <- tapply(dat$PA2, dat$Film, mean))
```


## ANOVA and SPSS Perspective

In the ANOVA world, I can express my hypotheses using three coding vectors as depicted in the Table \@ref(tab:contrasts1).
For example, I may investigate the difference between the last group "comedy" and the three other groups (H1).
Furthermore, I may test the difference between "horror" vs. "documentary"+"nature" (H2).
Lastly, I may test the difference between "documentary" and "nature".

```{r contrasts1, echo = FALSE}
tmp1 <- tibble::tribble(
    ~Hypothesis, ~c1, ~c2, ~c3, ~c4,
    "H1", -1, -1, -1, 3,
    "H2",  1, -2,  1, 0,
    "H3", -1,  0,  1, 0
)
knitr::kable(tmp1, caption = "ANOVA Contrasts", align="lrrrr")
```

Furthermore, one of the very few things that I like about SPSS is the fact that I can relatively easily define such contrasts using `UNIANOVA`:

```{r, eval = FALSE}
### SPSS Syntax ###
UNIANOVA PA2 BY Film
  /CONTRAST(Film) = SPECIAL(-1 -1 -1  3
                             1 -2  1  0
                            -1  0  1  0).
```

## Regression Perspective

In regression, we often deal with categorical predictors as well, and often a first choice is dummy coding (the default in R for unordered factors).
We all know how dummy coding works.
Here, we choose "comedy" as a reference category, and all other categories are contrasted with that reference category using the scheme depicted in Table \@ref(tab:dummy).
The estimates give us the difference between "comedy" and each of the three other groups.

```{r dummy, echo = FALSE}
# tmp1 <- tibble::tribble(
#     ~Group, ~d1, ~d2, ~d3,
#     "documentary", 1, 0, 0,
#     "horror",      0, 1, 0,
#     "nature",      0, 0, 1,
#     "comedy",      0, 0, 0)
tmp1 <- tibble::tribble(
    ~Group, ~d1, ~d2, ~d3,
    "horror",      1, 0, 0,
    "nature",      0, 1, 0,
    "comedy",      0, 0, 1,
    "documentary", 0, 0, 0)
knitr::kable(tmp1, caption = "Dummy Coding", align="lrrr")
```

## How to Combine the Perspectives?

Even though I was comfortable using either approach, the thing that always bugged me was that I personally wasn't able to fully bridge the two worlds. For example:

1. How can we use such schemes within the other framework? For example, plugging the dummy codes from Table \@ref(tab:dummy) into SPSS's UNIANOVA Syntax won't work.
2. How do we actually know the meaning of our estimates? For example, everybody knows that the estimate for `d1` above is the difference between "documentary" and "comedy". But why is this the case?

## Solution

The solution is simple, but:

```{r}
fortunes::fortune("done it.")
```

The solution is that the coding scheme used in SPSS above directly defines the **contrast matrix** $\mathbf{C}$ and this allows us to contrasts group means in a sensible way.
On the other hand, the dummy-coding scheme used above was a **coding matrix** $\mathbf{B}$ [@venables_coding_2018].
These are two different things and they are related in the following way:
$$\beta = \mathbf{C} \mu = \mathbf{B}^{-1} \mu.$$
That is, the estimates $\beta$ are the reparameterized group means $\mu$.
The contrasts in $\mathbf{C}$ directly specify the weights of the groups and are easily interpretable.
However, the interpretation of the codes in $\mathbf{B}$, for example, of the dummy codes used above, is not directly given, but only through the inverse of $\mathbf{B}$.

### Examples

#### Dummy Coding

Here, we use the default in R, namely, dummy coding, for the affect data.
Remember that we want to investigate whether positive affect ("PA2") differs between groups that watched one of four movies.
The interpretation of the results is straightforward:
The intercept is equal to the first group mean, and here it is significantly different from zero.
Furthermore, we observe that "comedy" in comparison to the reference category "documentary" leads to higher levels of positive effect (the difference is 4.478, p = .0001).

```{r}
contr.treatment(levels(dat$Film))

summary(lm(PA2 ~ Film, data = dat))

group_means

12.090 - 7.612       # == 'Filmhorror'
```

But why exactly are dummy codes interpreted in this way?
We learned in stats 101 how to interpret the $\mathbf{B}$ matrix depicted in Table \@ref(tab:dummy) (i.e., `contr.treatment()`).
But to really deduce the meaning, you have to take the inverse of the coding matrix $\mathbf{B}$. (The package **codingMatrices** has a wrapper around `solve()` with nice output, but I will use `solve()` here for transparency.)

```{r}
# we need to add a vector of 1's for the intercept
solve(cbind(Ave = 1, contr.treatment(4)))

# identical result but nicer ouput:
# codingMatrices::mean_contrasts(contr.treatment(4))
```

This returns the contrast matrix $\mathbf{C}$.
The first row gives you the interpretation of the intercept, and we see that it is $\beta_0 = 1\mu_1 + 0\mu_2  + 0\mu_3 + 0\mu_4$.
Likewise, from the second row, $\beta_1 = -1\mu_1 + 1\mu_2$; and so on.
The interpretation of this $\mathbf{C}$ matrix is much easier than that of the $\mathbf{B}$ matrix, which was only easy because we learned it by hard.

Lastly, this enables us to use dummy coding (or any other coding scheme that you learned by hard) in SPSS's `UNIANOVA`:

```{r, eval = FALSE}
### SPSS Syntax ###
UNIANOVA PA2 BY Film
  /CONTRAST(Film) = SPECIAL(-1  1  0  0
                            -1  0  1  0
                            -1  0  0  1).
```

#### Planned Comparisons/Contrast Coding

The whole story gets more interesting when trying to implement non-standard coding schemes within the regression framework.
Remember the hypotheses described in Table \@ref(tab:contrasts1) above; those are easily translated into a contrast matrix $\mathbf{C}$ called `C1` below.
In order to test the hypotheses within a regression framework, we have to calculate the $\mathbf{B}$ matrix (`B1`).
Furthermore, instead of using integral weights, I divide by the number of non-zero weights for easier interpretation of the estimates (but p-values are the same).

```{r}
tmp1 <- matrix(c( 1,  1,  1, 1,
                 -1, -1, -1, 3,
                 -1,  2, -1, 0,
                 -1,  0,  1, 0), 4, 4, byrow = TRUE)
C1 <- tmp1 / (4:1)
tmp1
round(C1, 2)

B1 <- solve(C1)
round(B1, 2)

colnames(B1) <- paste0("_Hyp", 0:3)
```

Finally, we can use the $\mathbf{B}$ matrix to specify the desired contrasts in a linear regression:

```{r}
summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = B1[, -1])))

mean(group_means)
group_means[[4]] - mean(group_means[1:3])
group_means[[2]] - mean(group_means[c(1, 3)])
group_means[[3]] - group_means[[1]]
```

As you can see, the following holds:

* $\beta_0 =  .25\mu_1 +.25\mu_2  +.25\mu_3 + .25\mu_4$,
* $\beta_1 = -.33\mu_1 -.33\mu_2  -.33\mu_3 +  \mu_4$,
* $\beta_2 = -.5 \mu_1 +   \mu_2  -.5\mu_3$,
* $\beta_3 = -   \mu_1            +   \mu_3$.

Thus, positive affect is 4.41 points higher after watching "comedy" compared to the three other films (p < .001). Moreover, positive affect is 0.32 points lower after "horror" compared to groups 1 and 3, but this is not significant (p = .742). And the difference between "documentary"" and "nature" is also non-significant (p = .760).


This gives us huge power. We can now test more specific hypotheses in a regression framework compared to what is possible using the software's defaults; for example, the hypotheses depicted in Table \@ref(tab:contrasts1) were easy to test using our custom matrices `B1` and `C1` but would be hard to test with built-in functions.
Furthermore, we know now how to deduce the meaning of the estimates instead of relying on textbook knowledge.

#### Helmert Coding

For further illustration, we will have a look at Helmert coding (`contr.helmert()`), which can be used to compare each level with the mean of the previous levels.
The $\mathbf{C}$ matrix `C2` already illustrates that, but it does not give an interpretation for $\beta_0$ and it does not allow to interpret the exact value of the other estimates.
This is given by its inverse `B2`, which shows that $\beta_0$ is again the mean of the group means (first row of `B2`). Furthermore, $\beta_3$ compares "comedy" to the three other groups (H1 in Table \@ref(tab:contrasts1)), and the p- and t-value (4.742) are the same as above.
However, the estimate of 1.10 has no easy interpretation, because it is $\beta_3 = -\frac{1}{12}\mu_1 - \frac{1}{12}\mu_2  - \frac{1}{12}\mu_3 + \frac{1}{4}\mu_4$ (with $\frac{1}{12} \approx 0.08$).
This was much easier to interpret above, where the estimate was 4.41, which was just the difference between "comedy" and the mean of the other three groups.

```{r}
(C2 <- contr.helmert(levels(dat$Film)))

B2 <- solve(cbind(1, C2))
round(B2, 2)

summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = contr.helmert)))

sum(B2[4, ] * group_means)
```

## Orthogonal and Nonorthognoal Contrasts

I did not find much literature that deduces the meaning of the $\textbf{B}$ matrix and explains the relationship between $\textbf{B}$ and $\textbf{C}$ except @venables_coding_2018.
Maybe because it is obvious for everyone except for me.
But maybe because the interpretation of orthogonal contrasts is relatively easy, because then $\textbf{B}$ and $\textbf{C}$ are structurally quite similar:

```{r}
# B matrix for Helmert coding
contr.helmert(5)
tmp1 <- solve(cbind(1, contr.helmert(5)))[-1, ]
# C matrix
round(t(tmp1), 2)
```

However, this is not the case for nonorthogonal contrasts like dummy- or effect-coding or other custom, nonorthogonal contrasts.
Apart from that, I can't see any clear advantage of orthogonal over nonorthogonal contrasts.
If you know better, I would be very happy if you could let me know.

```{r, include = F}
# grateful::cite_packages(all.pkg = FALSE, style = "apa", out.format = "md")
```

## References
