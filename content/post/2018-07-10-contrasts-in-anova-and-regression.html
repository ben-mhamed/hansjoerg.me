---
title: 'Categorical Predictors in ANOVA and Regression'
author: 'Hansjörg Plieninger'
date: '2018-07-10'
slug: contrasts-in-anova-and-regression
draft: no
summary: 'Data with categorical predictors can be analyzed in a regression framework as well as in an ANOVA framework. In either case, the grouping variable needs to be recoded and a default coding system for categorical variables is often dummy coding. Even though I usually prefer the more general regression framework, I like the ANOVA perspective because of its focus on meaningful coding schemes beyond dummy coding. Herein, I will illustrate how to use any coding scheme in either framework which will help you (a) to switch between ANOVA and regression and (b) use sensible comparisons of your groups.'
categories:
  - R
tags:
  - regression
  - ANOVA
  - categorical-encoding
  - experiment-design
  - R
  - SPSS
bibliography: ../../misc/lib.bib
csl: ../../misc/apa.csl
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
---


<div id="TOC">
<ul>
<li><a href="#regression-perspective">Regression Perspective</a></li>
<li><a href="#anova-and-spss-perspective">ANOVA and SPSS Perspective</a></li>
<li><a href="#how-to-combine-the-perspectives">How to Combine the Perspectives?</a></li>
<li><a href="#solution">Solution</a></li>
<li><a href="#examples">Examples</a><ul>
<li><a href="#example-data">Example data</a></li>
<li><a href="#dummy-coding">Dummy Coding</a></li>
<li><a href="#planned-comparisonscontrast-coding">Planned Comparisons/Contrast Coding</a></li>
<li><a href="#helmert-coding">Helmert Coding</a></li>
</ul></li>
<li><a href="#orthogonal-and-nonorthognoal-contrasts">Orthogonal and Nonorthognoal Contrasts</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>Data with categorical predictors such as groups, conditions, or countries can be analyzed in a regression framework as well as in an ANOVA framework. In either case, the grouping variable needs to be recoded, it cannot enter the model like a continuous predictor such as age or income. A default coding system for categorical variables is often dummy coding.</p>
<p>Even though I usually prefer the more general regression framework, I like the ANOVA perspective because of its focus on meaningful coding schemes beyond dummy coding. Herein, I will illustrate how to use any coding scheme in either framework which will help you (a) to switch between ANOVA and regression and (b) use sensible comparisons of your groups.</p>
<p>In the example below, I use four different groups of people that watched one of four movies, namely, documentary, horror, nature, or comedy.</p>
<!-- You can look at the problem of predicting a dependent variable using a categorical independent variable (factor) from a regression perspective or from an ANOVA perspective, and I usually prefer the former. However, what I always liked about the ANOVA perspective was the focus on meaningful coding schemes for categorical predictors with more than two levels. -->
<div id="regression-perspective" class="section level2">
<h2>Regression Perspective</h2>
<p>In regression, we often deal with categorical predictors and often a first choice is dummy coding (the default in R for unordered factors). We all know how dummy coding works. Here, we choose “documentary” as a reference category, and all other categories are contrasted with that reference category using the scheme depicted in Table <a href="#tab:dummy">1</a>. Then, the resulting three estimates give us the difference between “documentary” and each of the three other groups.</p>
<table>
<caption><span id="tab:dummy">Table 1: </span>Dummy Coding</caption>
<thead>
<tr class="header">
<th align="left">Group</th>
<th align="right">d1</th>
<th align="right">d2</th>
<th align="right">d3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">documentary</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">horror</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">nature</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">comedy</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
</div>
<div id="anova-and-spss-perspective" class="section level2">
<h2>ANOVA and SPSS Perspective</h2>
<p>In the ANOVA world or, for example, in experimental psychology, it is often desired to have more tailored comparisons to test specific hypotheses. This can be done using custom contrasts (planned comparisons) as depicted in Table <a href="#tab:contrasts1">2</a>. For example, I may investigate the difference between the last group “comedy” and the three other groups using the contrast (-1, -1, -1, 3) depicted in row H1. Furthermore, I may test the difference between “horror” vs. “documentary”+“nature” (H2). Lastly, I may test the difference between “documentary” and “nature”.</p>
<table>
<caption><span id="tab:contrasts1">Table 2: </span>ANOVA Contrasts</caption>
<thead>
<tr class="header">
<th align="left">Hypothesis</th>
<th align="right">c1</th>
<th align="right">c2</th>
<th align="right">c3</th>
<th align="right">c4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">H1</td>
<td align="right">-1</td>
<td align="right">-1</td>
<td align="right">-1</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="left">H2</td>
<td align="right">1</td>
<td align="right">-2</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">H3</td>
<td align="right">-1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>Furthermore, one of the very few things that I like about SPSS is the fact that I can easily define such contrasts using <code>UNIANOVA</code>:</p>
<pre class="r"><code>### SPSS Syntax ###
UNIANOVA Y BY X
  /CONTRAST(X) = SPECIAL(-1 -1 -1  3
                          1 -2  1  0
                         -1  0  1  0).</code></pre>
</div>
<div id="how-to-combine-the-perspectives" class="section level2">
<h2>How to Combine the Perspectives?</h2>
<p>Even though I was comfortable using either approach, the thing that always bugged me was that I personally wasn’t able to fully bridge the two worlds. For example:</p>
<ol style="list-style-type: decimal">
<li>How can we use such schemes within the other framework? For example, plugging the dummy codes from Table <a href="#tab:dummy">1</a> into SPSS’s <code>UNIANOVA</code> syntax won’t work.</li>
<li>How do we actually know the meaning of our estimates? For example, everybody knows that the estimate for the first dummy code “d1” above is the difference between “horror” and “documentary”. But why exactly is this the case?</li>
</ol>
</div>
<div id="solution" class="section level2">
<h2>Solution</h2>
<p>The solution is simple, but:</p>
<pre class="r"><code>fortunes::fortune(&quot;done it.&quot;)
#&gt; 
#&gt; It was simple, but you know, it&#39;s always simple when you&#39;ve done it.
#&gt;    -- Simone Gabbriellini (after solving a problem with a trick suggested
#&gt;       on the list)
#&gt;       R-help (August 2005)</code></pre>
<p>The solution is that the scheme used in Table <a href="#tab:contrasts1">2</a> above directly defines the <strong>contrast matrix</strong> <strong>C</strong> and this allows us to contrast group means in a sensible way. On the other hand, the dummy-coding scheme depicted in Table <a href="#tab:dummy">1</a> was a <strong>coding matrix</strong> <strong>B</strong> <span class="citation">(Venables, 2018)</span>. These are two different things and they are related in the following way: <span class="math display">\[\beta = \mathbf{C} \mu = \mathbf{B}^{-1} \mu.\]</span> That is, the estimates <span class="math inline">\(\beta\)</span> are the reparameterized group means <span class="math inline">\(\mu\)</span>. The contrasts in <strong>C</strong> directly specify the weights of the group means and are easily interpretable. However, the interpretation of the codes in <strong>B</strong> (e.g., of the dummy codes used above) is not directly given, but only through the inverse of <strong>B</strong>. This relationship between <strong>B</strong> and <strong>C</strong> and the interpretation of the regression coefficients <span class="math inline">\(\beta\)</span> will be illustrated in the following.</p>
</div>
<div id="examples" class="section level2">
<h2>Examples</h2>
<div id="example-data" class="section level3">
<h3>Example data</h3>
<p>I will used a data set called affect from the package psych. Therein, participants watched one of four different films and their positive affect (“PA2”) was assessed afterwards. The plot below shows clearly the effect of films on positive effect. To more formally investigate this effect (using regression or ANOVA), the categorical predictor film has to be recoded into three coding variables; herein, the three schemes dummy, contrast, and Helmert coding will be illustrated. More coding schemes are illustrated on this <a href="https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/">UCLA page</a>.</p>
<pre class="r"><code># install.packages(c(&quot;psych&quot;, &quot;ggplot2&quot;, &quot;dplyr&quot;, &quot;codingMatrices&quot;))
library(ggplot2)
#&gt; Warning: package &#39;ggplot2&#39; was built under R version 3.5.1
library(dplyr)
#&gt; Warning: package &#39;dplyr&#39; was built under R version 3.5.1

data(affect, package = &quot;psych&quot;)
# help(affect, package = &quot;psych&quot;)

affect$Film &lt;- factor(affect$Film, labels = c(&quot;documentary&quot;, &quot;horror&quot;, &quot;nature&quot;, &quot;comedy&quot;))

# Sample subgroups of equal size (n=50)
set.seed(123)
dat &lt;- affect %&gt;% 
    group_by(Film) %&gt;% 
    sample_n(50)

table(dat$Film)
#&gt; 
#&gt; documentary      horror      nature      comedy 
#&gt;          50          50          50          50

(group_means &lt;- tapply(dat$PA2, dat$Film, mean))
#&gt; documentary      horror      nature      comedy 
#&gt;       7.612       7.460       7.960      12.090

ggplot(dat, aes(y = PA2, x = Film)) + 
    stat_summary(fun.data = &quot;mean_cl_normal&quot;) +
    labs(y = &quot;Positive Affect (&#39;PA2&#39;)&quot;)</code></pre>
<p><img src="/post/2018-07-10-contrasts-in-anova-and-regression_files/figure-html/unnamed-chunk-3-1.png" width="80%" /></p>
</div>
<div id="dummy-coding" class="section level3">
<h3>Dummy Coding</h3>
<p>Here, we use the default in R, namely, dummy coding, for the affect data. Remember that we want to investigate whether positive affect (“PA2”) differs between groups that watched one of four films. The interpretation of the results is straightforward: The intercept is equal to the mean of the first group “documentary”, and here it is significantly different from zero. Furthermore, we observe that “comedy” in comparison to the reference category “documentary” leads to higher levels of positive effect (the difference is 4.478, p = .0001).</p>
<pre class="r"><code>contr.treatment(levels(dat$Film))
#&gt;             horror nature comedy
#&gt; documentary      0      0      0
#&gt; horror           1      0      0
#&gt; nature           0      1      0
#&gt; comedy           0      0      1

summary(lm(PA2 ~ Film, data = dat))
#&gt; 
#&gt; Call:
#&gt; lm(formula = PA2 ~ Film, data = dat)
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -12.090  -4.612  -0.536   3.942  18.040 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)   7.6120     0.8058   9.446  &lt; 2e-16 ***
#&gt; Filmhorror   -0.1520     1.1396  -0.133 0.894029    
#&gt; Filmnature    0.3480     1.1396   0.305 0.760407    
#&gt; Filmcomedy    4.4780     1.1396   3.929 0.000118 ***
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 5.698 on 196 degrees of freedom
#&gt; Multiple R-squared:  0.1038, Adjusted R-squared:  0.09005 
#&gt; F-statistic: 7.564 on 3 and 196 DF,  p-value: 8.184e-05

group_means
#&gt; documentary      horror      nature      comedy 
#&gt;       7.612       7.460       7.960      12.090

12.090 - 7.612       # == &#39;Filmcomedy&#39;
#&gt; [1] 4.478</code></pre>
<p>But why exactly are the four estimates interpreted in this way? We learned in stats 101 how to interpret the <strong>B</strong> matrix depicted in Table <a href="#tab:dummy">1</a> (i.e., <code>contr.treatment()</code>). But to really deduce the meaning, you have to take the inverse of the coding matrix <strong>B</strong>, which can be done using <code>solve()</code>. (The package <strong>codingMatrices</strong> has a wrapper around <code>solve()</code> with nice output, but I will use <code>solve()</code> here for transparency.)</p>
<pre class="r"><code># we need to add a vector of 1&#39;s for the intercept
solve(cbind(b0 = 1, contr.treatment(4)))
#&gt;     1 2 3 4
#&gt; b0  1 0 0 0
#&gt; 2  -1 1 0 0
#&gt; 3  -1 0 1 0
#&gt; 4  -1 0 0 1

# identical result but nicer ouput:
# codingMatrices::mean_contrasts(contr.treatment(4))</code></pre>
<p>This returns the contrast matrix <strong>C</strong>. The first row gives you the interpretation of the intercept, and we see that it is <span class="math inline">\(\beta_0 = 1\mu_1 + 0\mu_2 + 0\mu_3 + 0\mu_4\)</span>, namely, the mean of the first group. Likewise, from the second row, <span class="math inline">\(\beta_1 = -1\mu_1 + 1\mu_2\)</span>, that is, the difference between group two and group one; and so on. The interpretation of this <strong>C</strong> matrix is much easier than that of the <strong>B</strong> matrix, which was only easy because we learned it by hard.</p>
<p>Lastly, this enables us to use dummy coding (or any other coding scheme that you learned by hard) in SPSS’s <code>UNIANOVA</code>:</p>
<pre class="r"><code>### SPSS Syntax ###
UNIANOVA PA2 BY Film
  /CONTRAST(Film) = SPECIAL(-1  1  0  0
                            -1  0  1  0
                            -1  0  0  1).</code></pre>
</div>
<div id="planned-comparisonscontrast-coding" class="section level3">
<h3>Planned Comparisons/Contrast Coding</h3>
<p>The whole story gets more interesting when trying to implement non-standard coding schemes within the regression framework. Remember the hypotheses described in Table <a href="#tab:contrasts1">2</a> above; those are easily translated into a contrast matrix <strong>C</strong> called <code>C1</code> below. In order to test the hypotheses within a regression framework, we have to invert <code>C1</code> to get the <strong>B</strong> matrix <code>B1</code>. Furthermore, instead of using integral weights, I divide by the number of non-zero weights for easier interpretation of the estimates (but p-values are the same).</p>
<pre class="r"><code>tmp1 &lt;- matrix(c( 1,  1,  1, 1,
                 -1, -1, -1, 3,
                 -1,  2, -1, 0,
                 -1,  0,  1, 0), 4, 4, byrow = TRUE)
C1 &lt;- tmp1 / (4:1)
tmp1
#&gt;      [,1] [,2] [,3] [,4]
#&gt; [1,]    1    1    1    1
#&gt; [2,]   -1   -1   -1    3
#&gt; [3,]   -1    2   -1    0
#&gt; [4,]   -1    0    1    0
round(C1, 2)
#&gt;       [,1]  [,2]  [,3] [,4]
#&gt; [1,]  0.25  0.25  0.25 0.25
#&gt; [2,] -0.33 -0.33 -0.33 1.00
#&gt; [3,] -0.50  1.00 -0.50 0.00
#&gt; [4,] -1.00  0.00  1.00 0.00

B1 &lt;- solve(C1)
round(B1, 2)
#&gt;      [,1]  [,2]  [,3] [,4]
#&gt; [1,]    1 -0.25 -0.33 -0.5
#&gt; [2,]    1 -0.25  0.67  0.0
#&gt; [3,]    1 -0.25 -0.33  0.5
#&gt; [4,]    1  0.75  0.00  0.0

colnames(B1) &lt;- paste0(&quot;_Hyp&quot;, 0:3)</code></pre>
<p>Finally, we can use the <strong>B</strong> matrix to test the desired contrasts in a linear regression:</p>
<pre class="r"><code>summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = B1[, -1])))
#&gt; 
#&gt; Call:
#&gt; lm(formula = PA2 ~ Film, data = dat, contrasts = list(Film = B1[, 
#&gt;     -1]))
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -12.090  -4.612  -0.536   3.942  18.040 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)   8.7805     0.4029  21.793  &lt; 2e-16 ***
#&gt; Film_Hyp1     4.4127     0.9305   4.742 4.06e-06 ***
#&gt; Film_Hyp2    -0.3260     0.9869  -0.330    0.742    
#&gt; Film_Hyp3     0.3480     1.1396   0.305    0.760    
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 5.698 on 196 degrees of freedom
#&gt; Multiple R-squared:  0.1038, Adjusted R-squared:  0.09005 
#&gt; F-statistic: 7.564 on 3 and 196 DF,  p-value: 8.184e-05

mean(group_means)
#&gt; [1] 8.7805
group_means[[4]] - mean(group_means[1:3])
#&gt; [1] 4.412667
group_means[[2]] - mean(group_means[c(1, 3)])
#&gt; [1] -0.326
group_means[[3]] - group_means[[1]]
#&gt; [1] 0.348</code></pre>
<p>As you can see, the following holds:</p>
<ul>
<li><span class="math inline">\(\beta_0 = .25\mu_1 +.25\mu_2 +.25\mu_3 + .25\mu_4\)</span>,</li>
<li><span class="math inline">\(\beta_1 = -.33\mu_1 -.33\mu_2 -.33\mu_3 + \mu_4\)</span>,</li>
<li><span class="math inline">\(\beta_2 = -.5 \mu_1 + \mu_2 -.5\mu_3\)</span>,</li>
<li><span class="math inline">\(\beta_3 = - \mu_1 + \mu_3\)</span>.</li>
</ul>
<p>Thus, positive affect is 4.41 points higher after watching “comedy” compared to the three other films (p &lt; .001). Moreover, positive affect is 0.33 points lower after “horror” compared to groups 1 and 3, but this is not significant (p = .742). And the difference between “documentary”&quot; and “nature” is also non-significant (p = .760).</p>
<p>This gives us huge power. We can now test more specific hypotheses in a regression framework compared to what is possible using the software’s defaults. For example, the hypotheses depicted in Table <a href="#tab:contrasts1">2</a> were easy to test using our custom matrices <code>B1</code> and <code>C1</code> but would be hard to test with built-in functions. Furthermore, we know now how to deduce the meaning of the estimates instead of relying on textbook knowledge.</p>
</div>
<div id="helmert-coding" class="section level3">
<h3>Helmert Coding</h3>
<p>For further illustration, we will have a look at Helmert coding (<code>contr.helmert()</code>), which can be used to compare each level with the mean of the previous levels. The <strong>C</strong> matrix <code>C2</code> below already illustrates that, but it does not give an interpretation for <span class="math inline">\(\beta_0\)</span> and it does not allow to interpret the exact value of the other estimates. This is given by its inverse <code>B2</code>, which shows that <span class="math inline">\(\beta_0\)</span> is again the mean of the group means (first row of <code>B2</code>). Furthermore, <span class="math inline">\(\beta_3\)</span> compares “comedy” to the three other groups (H1 in Table <a href="#tab:contrasts1">2</a>), and the p- and t-value (4.742) are the same as above. However, the estimate of 1.10 has no easy interpretation, because it is <span class="math inline">\(\beta_3 = -0.08\mu_1 - 0.08\mu_2 - 0.08\mu_3 + 0.25\mu_4\)</span>. This was much easier to interpret above, where the estimate was 4.41, which was just the difference between “comedy” and the mean of the other three groups.</p>
<pre class="r"><code>(C2 &lt;- contr.helmert(levels(dat$Film)))
#&gt;             [,1] [,2] [,3]
#&gt; documentary   -1   -1   -1
#&gt; horror         1   -1   -1
#&gt; nature         0    2   -1
#&gt; comedy         0    0    3

B2 &lt;- solve(cbind(1, C2))
round(B2, 2)
#&gt;      documentary horror nature comedy
#&gt; [1,]        0.25   0.25   0.25   0.25
#&gt; [2,]       -0.50   0.50   0.00   0.00
#&gt; [3,]       -0.17  -0.17   0.33   0.00
#&gt; [4,]       -0.08  -0.08  -0.08   0.25

summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = contr.helmert)))
#&gt; 
#&gt; Call:
#&gt; lm(formula = PA2 ~ Film, data = dat, contrasts = list(Film = contr.helmert))
#&gt; 
#&gt; Residuals:
#&gt;     Min      1Q  Median      3Q     Max 
#&gt; -12.090  -4.612  -0.536   3.942  18.040 
#&gt; 
#&gt; Coefficients:
#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    
#&gt; (Intercept)   8.7805     0.4029  21.793  &lt; 2e-16 ***
#&gt; Film1        -0.0760     0.5698  -0.133    0.894    
#&gt; Film2         0.1413     0.3290   0.430    0.668    
#&gt; Film3         1.1032     0.2326   4.742 4.06e-06 ***
#&gt; ---
#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
#&gt; 
#&gt; Residual standard error: 5.698 on 196 degrees of freedom
#&gt; Multiple R-squared:  0.1038, Adjusted R-squared:  0.09005 
#&gt; F-statistic: 7.564 on 3 and 196 DF,  p-value: 8.184e-05

sum(B2[4, ] * group_means)
#&gt; [1] 1.103167</code></pre>
</div>
</div>
<div id="orthogonal-and-nonorthognoal-contrasts" class="section level2">
<h2>Orthogonal and Nonorthognoal Contrasts</h2>
<p>I did not find much literature that deduces the meaning of the <strong>B</strong> matrix and explains the relationship between <strong>B</strong> and <strong>C</strong> except <span class="citation">Venables (2018)</span>. Maybe because it is obvious for everyone except for me. But maybe because people are taught to use either standard schemes (like dummy coding) or orthogonal contrasts. Orthogonal contrasts have the advantage that they can be interpreted using either the <strong>B</strong> or the <strong>C</strong> matrix, because they are structurally quite similar, and thus no one needs to know about or calculate an inverse:</p>
<pre class="r"><code># B matrix for Helmert coding
contr.helmert(5)
#&gt;   [,1] [,2] [,3] [,4]
#&gt; 1   -1   -1   -1   -1
#&gt; 2    1   -1   -1   -1
#&gt; 3    0    2   -1   -1
#&gt; 4    0    0    3   -1
#&gt; 5    0    0    0    4

# C matrix
tmp1 &lt;- solve(cbind(1, contr.helmert(5)))[-1, ]
round(t(tmp1), 2)
#&gt;   [,1]  [,2]  [,3]  [,4]
#&gt; 1 -0.5 -0.17 -0.08 -0.05
#&gt; 2  0.5 -0.17 -0.08 -0.05
#&gt; 3  0.0  0.33 -0.08 -0.05
#&gt; 4  0.0  0.00  0.25 -0.05
#&gt; 5  0.0  0.00  0.00  0.20</code></pre>
<p>I don’t know of any other clear advantages of orthogonal over nonorthogonal contrasts. If you know better, I would be very happy if you could let me know.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-venables_coding_2018">
<p>Venables, B. (2018). <em>Coding matrices, contrast matrices and linear models</em>. Retrieved from <a href="https://cran.r-project.org/web/packages/codingMatrices/vignettes/" class="uri">https://cran.r-project.org/web/packages/codingMatrices/vignettes/</a></p>
</div>
</div>
</div>
