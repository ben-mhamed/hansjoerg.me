---
title: Contrasts in ANOVA and Regression
author: HansjÃ¶rg
date: '2018-02-22'
slug: contrasts-in-anova-and-regression
categories: []
draft: true
tags:
  - R
  - SPSS
  - regression
  - ANOVA
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = T, comment = "#>")
```

You can look at the problem of predicting a dependent variable using (categorical) independent variables from a regression of from an ANOVA perspective, and I usually prefer the former. However, what I always liked about the ANOVA perspective was the focus on meaningful coding schemes for categorical predictors.

## Example data

Participants watched one of four different films and their positive affect ("PA2") was assessed afterwards. To investigate the effect of films on affect, the categorical predictor has to be recoded into 3 coding variables.

```{r, message = F}
library(dplyr)
# library(fractional)
# library(codingMatrices)
# library(ggplot2)

data(affect, package = "psych")
# help(affect, package = "psych")

affect$Film <- factor(affect$Film, labels = c("documentary", "horror", "nature", "comedy"))

# Sample subgroups of equal size (n=50)
set.seed(123)
dat <- affect %>% 
    group_by(Film) %>% 
    sample_n(50)

table(dat$Film)

(group_means <- tapply(dat$PA2, dat$Film, mean))
```


## ANOVA and SPSS Perspective

In the ANOVA world, I can express my hypotheses using three coding vectors as depicted in the table. For example, I may investigate the difference between "comedy" vs. the three other groups (H1). I may test the difference between "horror" vs. "documentary"/"nature" (H2). And I may test the difference between "documentary" and "nature".

```{r, echo = FALSE}
tmp1 <- tibble::tribble(
    ~Hypothesis, ~c1, ~c2, ~c3, ~c4,
    "H1", -1, -1, -1, 3,
    "H2",  1, -2,  1, 0,
    "H3", -1,  0,  1, 0
)
knitr::kable(tmp1)
```

Furthermore, one of the very few things that I like about SPSS is the fact that I can relatively easily define such contrasts using `UNIANOVA`:

```{r, eval = FALSE}
### SPSS Syntax ###
UNIANOVA PA2 BY Film
  /CONTRAST(Film) = SPECIAL(-1 -1 -1  3
                             1 -2  1  0
                            -1  0  1  0).
```

## Regression Perspective

In regression, we often deal with categorical predictors as well, and often a first choice is dummy coding (the default in R for unordered factors). We all know how dummy coding works. Here, choosing "comedy" as a reference category, we can contrast each category with that using the following scheme, and the estimates give us the difference between "comedy" and each of the three other groups.

```{r, echo = FALSE}
tmp1 <- tibble::tribble(
    ~Group, ~d1, ~d2, ~d3,
    "documentary", 1, 0, 0,
    "horror",      0, 1, 0,
    "nature",      0, 0, 1,
    "comedy",      0, 0, 0)
knitr::kable(tmp1)
```

## How to Combine the Perspectives?

Even though I was comfortable using either approach, the thing that always bugged me was that I personally wasn't able to fully bridge the two worlds. For example:

1. How can we use such schemes within the other framework? For example, plugging those dummy codes into SPSS's UNIANOVA Syntax won't work.
2. How do we actually know the meaning of our estimates? For example, everybody knows that the estimate for `d1` above is the difference between "documentary" and "comedy", but why is this the case?

## Solution

The solution is simple, but:

```{r}
fortunes::fortune("done it.")
```

The solution is that the coding scheme used in SPSS above directly defines the **contrast matrix** $\mathbf{C}$ and this allows us to contrasts group means in a sensible way. On the other hand, the dummy-coding scheme used above was a **coding matrix** $\mathbf{B}$. These are two different things and they are related in the following way:
$$\beta = \mathbf{C} \mu = \mathbf{B}^{-1} \mu.$$
That is, the estimates are the reparameterized group means. The contrasts in $\mathbf{C}$ directly specify the weights of the groups and are easily interpretable. However, only the inverse of $\mathbf{B}$ and not the codes themselves give us the meaning of the estimates.

### Examples

#### Dummy Coding

Using dummy coding for the affect data gives the following results:

```{r}
contr.treatment(4)

summary(lm(PA2 ~ Film, data = dat))

group_means

7.46 - 7.612       # == 'Filmhorror'
```

Everybody knows that the intercept is equal to the first group mean, and so on and so forth. But why is this exactly the case. This can be seen when taking the inverse of the coding matrix (we need to add a vector of 1's for the intercept):

```{r}
solve(cbind(Ave = 1, contr.treatment(4)))

# codingMatrices::mean_contrasts(contr.treatment(4))
```

This is the contrast matrix $\mathbf{C}$. The first row gives you the interpretation of the intercept, and we see that it's $\beta_0 = 1\mu_1 + 0\mu_2  + 0\mu_3 + 0\mu_4$. Likewise, $\beta_1 = -1\mu_1 + 1\mu_2$. (So if you want to use hand-coded dummy coding in SPSS, you need to use the last three rows of this $\mathbf{C}$ matrix above.)

#### Planned Comparisons/Contrast Coding

The whole story gets more interesting when trying to implement non-standard coding schemes within the regression framework. Remember the hypotheses described in the first table above. When I want to specify those within a regression framework, I have to take the inverse. Furthermore, instead of using the integral weights from above, I divide by the number of non-zero weights for easier interpretation of the estimates (but p-values are the same).

```{r}
(B1 <- matrix(c( 1,  1,  1, 1,
                -1, -1, -1, 3,
                 1, -2,  1, 0,
                -1,  0,  1, 0), 4, 4, byrow = TRUE))
B1 <- B1 / (4:1)

C <- solve(B1)[, -1]
colnames(C) <- paste0("_Hyp", 1:3)

summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = C)))

group_means[4] - mean(group_means[1:3])
```

As you can see, the following holds: $\beta_1 = -.33\mu_1 -.33\mu_2  -.33\mu_3 + 1\mu_4$. And this gives us huge power. We can now test more specific hypotheses in a regression framework compared to what is possible using the software's defaults. And we know now how to interpret the estimates in relationship to the group means instead of relying on textbook knowledge. For Helmert coding (`contr.helmert()`), for example, the last contrasts tests my first hypothesis described above (and again we have t=4.742). However, the estimate of 1.1032 has no direct or intuitive meaning because it's $\beta_3 = -\frac{1}{12}\mu_1 - \frac{1}{12}\mu_2  - \frac{1}{12}\mu_3 + \frac{1}{4}\mu_4$.

```{r}
### Helmert ###
summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = contr.helmert)))

contr.helmert(4)

C1 <- solve(cbind(Ave = 1, contr.helmert(4)))
fractional::fractional(C1)

# codingMatrices::mean_contrasts(contr.helmert(4))

C1 %*% group_means
```

## Orthogonal Contrasts

This knowledge alows us to test wheter our contrasts are orthogonal, because this is required for the contrasts in $\textbf{C}$ and not for the coding vectors in $\textbf{B}$.


```{r, eval = F}
### Dummy ###
summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = contr.treatment)))
summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = code_control)))

### Effect ###
summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = contr.sum)))
summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = code_deviation)))

### Difference ###
summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = contr.diff)))
summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = code_diff)))

### Helmert ###
summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = contr.helmert)))
summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = code_helmert)))


x1 <- contr.helmert(4)
x1[, 1] <- c(-3, 1, 1, 1)
x1[, 2] <- c( 0, -2, 1, 1)
x1[, 3] <- c( 0,  0, -1, 1)

summary(lm(PA2 ~ Film, data = dat, contrasts = list(Film = x1)))

```

```{r, include = F}
grateful::cite_packages(all.pkg = FALSE, style = "apa", out.format = "md")
```

